{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "information_extraction_example_solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2oxBO-8WS7S"
      },
      "outputs": [],
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy validate"
      ],
      "metadata": {
        "id": "fY4YYpRYgpxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "HLk-fm29WXjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install feedparser"
      ],
      "metadata": {
        "id": "pNne1NP0oyuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher, DependencyMatcher\n",
        "from spacy.tokens import Span \n",
        "from spacy import displacy\n",
        "from spacy.pipeline import merge_entities, merge_noun_chunks\n",
        "\n",
        "import inflect\n",
        "\n",
        "import sqlite3\n",
        "\n",
        "import feedparser"
      ],
      "metadata": {
        "id": "K-JHCgC4XTRV"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InformationExtractor:\n",
        "\n",
        "  def __init__(self, text):\n",
        "    '''Extract Triples, Hearst-Patterns, and Enumerations out of any text'''\n",
        "\n",
        "    self.text = text\n",
        "    self.qa_pairs = []\n",
        "    # initialize inflecter\n",
        "    self.p = inflect.engine()\n",
        "    # load the spacy language model\n",
        "    # more: https://spacy.io/usage/models\n",
        "    self.nlp = spacy.load('en_core_web_sm')\n",
        "    self.nlp.add_pipe('merge_entities')\n",
        "    self.nlp.add_pipe('merge_noun_chunks')\n",
        "\n",
        "    # initialize the depency matcher\n",
        "    self.dep_matcher = DependencyMatcher(self.nlp.vocab)\n",
        "\n",
        "    # X such as Y\n",
        "    self.hearst1 = [{'DEP':'amod', 'OP':\"?\"},\n",
        "                    {'POS':'NOUN'}, \n",
        "                    {'LOWER': 'such'}, \n",
        "                    {'LOWER': 'as'}, \n",
        "                    {'POS': {'IN': ['PROPN', 'NOUN']}}]\n",
        "    # X and/or Y\n",
        "    self.hearst2 = [{'DEP':'amod', 'OP':\"?\"}, \n",
        "                    {'POS':'NOUN'}, \n",
        "                    {'LOWER': 'and', 'OP':\"?\"}, \n",
        "                    {'LOWER': 'or', 'OP':\"?\"}, \n",
        "                    {'LOWER': 'other'}, \n",
        "                    {'POS': 'NOUN'}]\n",
        "    # X, including Y\n",
        "    self.hearst3 = [{'DEP':'nummod','OP':\"?\"},\n",
        "                    {'DEP':'amod','OP':\"?\"},\n",
        "                    {'POS':'NOUN'}, \n",
        "                    {'IS_PUNCT': True}, \n",
        "                    {'LOWER': 'including'}, \n",
        "                    {'DEP':'nummod','OP':\"?\"}, \n",
        "                    {'DEP':'amod','OP':\"?\"}, \n",
        "                    {'POS':'NOUN'}]\n",
        "    # X, especially Y\n",
        "    self.hearst4 = [{'DEP':'nummod','OP':\"?\"}, \n",
        "                    {'DEP':'amod','OP':\"?\"}, \n",
        "                    {'POS':'NOUN'}, \n",
        "                    {'IS_PUNCT': True}, \n",
        "                    {'LOWER': 'especially'}, \n",
        "                    {'DEP':'nummod','OP':\"?\"}, \n",
        "                    {'DEP':'amod','OP':\"?\"}, \n",
        "                    {'POS':'NOUN'}] \n",
        "\n",
        "    self.matcher = Matcher(self.nlp.vocab)\n",
        "    self.matcher.add('such as ', [self.hearst1])\n",
        "    self.matcher.add('and/or ', [self.hearst2])\n",
        "    self.matcher.add(', including ', [self.hearst3])\n",
        "    self.matcher.add(', especially ', [self.hearst4])\n",
        "\n",
        "    self.doc = self.nlp(text)\n",
        "    self.matches = self.matcher(self.doc)\n",
        "\n",
        "  @property\n",
        "  def pairs(self):\n",
        "    return self.qa_pairs\n",
        "\n",
        "  def extract(self):\n",
        "    for sent in self.doc.sents:\n",
        "      self._get_hearst()\n",
        "      self._get_triple()\n",
        "\n",
        "  def _get_hearst(self):\n",
        "    sing_result = []\n",
        "    \n",
        "    for match_id, start, end in self.matches:\n",
        "      span = self.doc[start:end]\n",
        "\n",
        "      for token in span.subtree:\n",
        "        if str(token.morph) == 'Number=Plur':\n",
        "          sing_result.append(self.p.singular_noun(token.text))\n",
        "        else:\n",
        "          sing_result.append(token.text)\n",
        "\n",
        "      result = ' '.join(sing_result)\n",
        "\n",
        "      match_text = self.nlp.vocab.strings[match_id]\n",
        "      if match_text == 'and/or ':\n",
        "        result = result.replace('and', 'or', 1)\n",
        "        match_text = 'or '\n",
        "      if match_text == ', including ':\n",
        "        result = result.replace('_', ',', 1)\n",
        "      try:\n",
        "        pc = result.split(match_text, 1)\n",
        "        parent = pc[0]\n",
        "        child = pc[1]\n",
        "        children = child.replace(',', '').replace('and ', '').replace('or ', '').split()\n",
        "      except IndexError:\n",
        "        print(result)\n",
        "        children = None\n",
        "\n",
        "\n",
        "      # form qa-pairs\n",
        "      if children is not None:\n",
        "        for c in children:\n",
        "          a = c + ' is a ' + parent.rstrip() + '.'\n",
        "          q = 'What is ' + c + '?'\n",
        "          self.qa_pairs.append((q,a))\n",
        "          print(q,a)\n",
        "\n",
        "  def _get_triple(self):\n",
        "    # use dependency parse\n",
        "    passive = False\n",
        "    triple = {'subj': '', 'pred': '', 'obj': ''}\n",
        "    for token in self.doc:\n",
        "      if token.dep_.find('subjpass') == True:\n",
        "        passive = True\n",
        "\n",
        "    if passive == True:\n",
        "      pattern = [\n",
        "      {\n",
        "          'RIGHT_ID': 'pred',\n",
        "          'RIGHT_ATTRS': {'DEP': 'ROOT'}\n",
        "      },\n",
        "      {\n",
        "          'LEFT_ID': 'pred',\n",
        "          'REL_OP': '>',\n",
        "          'RIGHT_ID': 'obj',\n",
        "          'RIGHT_ATTRS': {'DEP': 'nsubjpass'}\n",
        "      },\n",
        "      {\n",
        "          'LEFT_ID': 'pred',\n",
        "          'REL_OP': '>>',\n",
        "          'RIGHT_ID': 'subj',\n",
        "          'RIGHT_ATTRS': {\"DEP\": {'IN': ['pobj']}},\n",
        "      }\n",
        "  ]\n",
        "      self.dep_matcher.add('PASSIVE', [pattern])\n",
        "    else:\n",
        "      pattern = [\n",
        "      {\n",
        "          'RIGHT_ID': 'pred',\n",
        "          'RIGHT_ATTRS': {'DEP': 'ROOT'}\n",
        "      },\n",
        "      {\n",
        "          'LEFT_ID': 'pred',\n",
        "          'REL_OP': '>>',\n",
        "          'RIGHT_ID': 'obj',\n",
        "          'RIGHT_ATTRS': {'DEP': {'IN': ['pobj', 'dobj']}}\n",
        "\n",
        "      },\n",
        "      {\n",
        "        'LEFT_ID': 'pred',\n",
        "        'REL_OP': '>',\n",
        "        'RIGHT_ID': 'subj',\n",
        "        'RIGHT_ATTRS': {'DEP': {'IN': ['pobj', 'nsubj']}}\n",
        "      }\n",
        "  ]\n",
        "      self.dep_matcher.add('ACTIVE', [pattern])\n",
        "\n",
        "    matches = self.dep_matcher(self.doc)\n",
        "    \n",
        "    #TODO: extend matches to catch enumerations\n",
        "    try:\n",
        "      match_id, token_ids = matches[0]\n",
        "      for i in range(len(token_ids)):\n",
        "        triple[pattern[i][\"RIGHT_ID\"]] = self.doc[token_ids[i]]\n",
        "      print(triple)\n",
        "      \n",
        "      # ask for subject\n",
        "      # possible ent types: https://towardsdatascience.com/explorations-in-named-entity-recognition-and-was-eleanor-roosevelt-right-671271117218\n",
        "      if triple['subj'].ent_type_ in ['PERSON', 'GPE', 'ORG', 'NORP']:\n",
        "        q = 'Who ' + triple['pred'].text + ' ' + triple['obj'].text + '?'\n",
        "        a = triple['subj'].text\n",
        "        self.qa_pairs.append((q,a))\n",
        "      elif triple['subj'].ent_type_ in ['DATE', 'TIME']:\n",
        "        q = 'When ' + triple['pred'].text + ' ' + triple['obj'].text + '?'\n",
        "        a = triple['subj'].text\n",
        "        self.qa_pairs.append((q,a))\n",
        "      elif triple['subj'].ent_type_ in ['LOC', 'FAC']:\n",
        "        q = 'Where ' + triple['pred'].text + ' ' + triple['obj'].text + '?'\n",
        "        a = triple['subj'].text\n",
        "        self.qa_pairs.append((q,a))\n",
        "      elif triple['subj'].ent_type_ in ['MONEY', 'QUANTITY', 'PERCENT']:\n",
        "        q = 'How much ' + triple['pred'].text + ' ' + triple['obj'].text + '?'\n",
        "        a = triple['subj'].text\n",
        "        self.qa_pairs.append((q,a))\n",
        "      elif triple['subj'].ent_type_ in ['PRODUCT', 'EVENT', 'WORK_OF_ART']:\n",
        "        q = 'What ' + triple['pred'].text + ' ' + triple['obj'].text + '?'\n",
        "        a = triple['subj'].text\n",
        "        self.qa_pairs.append((q,a))\n",
        "      elif triple['subj'].ent_type_ in ['CARDINAL']:\n",
        "        q = 'How many ' + triple['pred'].text + ' ' + triple['obj'].text + '?'\n",
        "        a = triple['subj'].text\n",
        "        self.qa_pairs.append((q,a))\n",
        "      elif triple['subj'].ent_type_ in ['LANGUAGE']:\n",
        "        q = 'What language ' + triple['pred'].text + ' ' + triple['obj'].text + '?'\n",
        "        a = triple['subj'].text\n",
        "        self.qa_pairs.append((q,a))\n",
        "      elif triple['subj'].ent_type_ in ['LAW']:\n",
        "        q = 'What law ' + triple['pred'].text + ' ' + triple['obj'].text + '?'\n",
        "        a = triple['subj'].text\n",
        "        self.qa_pairs.append((q,a))\n",
        "      else:\n",
        "        q = 'What ' + triple['pred'].text + ' ' + triple['obj'].text + '?'\n",
        "        a = triple['subj'].text\n",
        "        self.qa_pairs.append((q,a))\n",
        "\n",
        "    except IndexError:\n",
        "      self.visualize()\n",
        "\n",
        "  def get_gramm_info(self):\n",
        "    for token in self.doc:\n",
        "      print(token.text, token.dep_, token.pos_)\n",
        "\n",
        "  def visualize(self):\n",
        "    displacy.render(self.doc, jupyter='true')\n",
        "    # # in case of vscode\n",
        "    # svg = displacy.render(self.doc, style='dep')\n",
        "    \n",
        "    # with open('sent.svg', 'w', encoding='utf-8') as outfile:\n",
        "    #   outfile.write(svg)"
      ],
      "metadata": {
        "id": "W87qSMvez38X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feed = feedparser.parse('https://www.democracynow.org/democracynow.rss')\n",
        "for entry in feed.entries:\n",
        "  print(entry.summary)"
      ],
      "metadata": {
        "id": "Bxv3uqwXo7p3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "feed = feedparser.parse('https://www.democracynow.org/democracynow.rss')\n",
        "extracts = []\n",
        "entry_counter = 0\n",
        "\n",
        "for entry in feed.entries:\n",
        "  entry_counter += 1\n",
        "  text = entry.summary\n",
        "  doc = nlp(text)\n",
        "  for sent in doc.sents:\n",
        "    ext = InformationExtractor(sent.text)\n",
        "    # ext.visualize()\n",
        "      \n",
        "    ext.extract()\n",
        "    for qa in ext.qa_pairs:\n",
        "      extracts.append(qa)\n",
        "\n",
        "print(len(extracts))\n",
        "print(entry_counter)"
      ],
      "metadata": {
        "id": "9KJGCyGOz5we"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "RfVyfQ0nFwbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir gdrive/MyDrive/qa_pairs"
      ],
      "metadata": {
        "id": "pzMffqaiFe9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# connect to database, if not existing, a new one will be created\n",
        "conn = sqlite3.connect('gdrive/MyDrive/qa_pairs/qa_pairs.db')\n",
        "\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# create a table\n",
        "conn.execute('''CREATE TABLE QA\n",
        "         (ID INT PRIMARY KEY NOT NULL,\n",
        "         QUESTION TEXT NOT NULL,\n",
        "         ANSWER TEXT NOT NULL);''')"
      ],
      "metadata": {
        "id": "kJsdi9ofGVi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write contents of extracts into database\n",
        "index = 0\n",
        "for q, a in extracts:\n",
        "  index += 1\n",
        "  query = '''INSERT INTO QA\n",
        "  (ID, QUESTION, ANSWER)\n",
        "  VALUES\n",
        "  (?, ?, ?)'''\n",
        "  data = (index, q, a)\n",
        "  cursor.execute(query, data)\n",
        "  conn.commit()"
      ],
      "metadata": {
        "id": "s0o57jJjI8Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at the results\n",
        "cursor.execute('''SELECT * from QA''')\n",
        "records = cursor.fetchall()\n",
        "for row in records:\n",
        "  print(row)"
      ],
      "metadata": {
        "id": "bNEMLwq7L2xM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create .csv file to use for fine tuning\n",
        "import csv\n",
        "with open('gdrive/MyDrive/qa_pairs/qa_pairs.csv', 'w', newline='') as csvfile:\n",
        "  fieldnames = ['prompt', 'completion']\n",
        "  writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "  writer.writeheader()\n",
        "  cursor.execute('''SELECT * from QA''')\n",
        "  records = cursor.fetchall()\n",
        "  for row in records:\n",
        "    ID, prompt, completion = row\n",
        "    writer.writerow({'prompt': prompt, 'completion': completion})"
      ],
      "metadata": {
        "id": "13TOLrTttVDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "fE0pVGYICV6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai"
      ],
      "metadata": {
        "id": "NmC8l7iTx6LY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!openai tools fine_tunes.prepare_data -f gdrive/MyDrive/qa_pairs/qa_pairs.csv"
      ],
      "metadata": {
        "id": "jEyypfFTydzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!export OPENAI_API_KEY=\"<YOUR KEY>\""
      ],
      "metadata": {
        "id": "MrfqXS4AzB-0"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!openai api fine_tunes.create -t \"gdrive/MyDrive/qa_pairs/qa_pairs_prepared.jsonl\""
      ],
      "metadata": {
        "id": "drGnnhBzCJGm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}